{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5627a2ad",
   "metadata": {},
   "source": [
    "### Using the Greedy Tokenizer to encode text\n",
    "\n",
    "In the first example, we use the token sets we obtained.\n",
    "In the second example, we use existing token sets from BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712990c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "import os\n",
    "from pcatt import greedy_builder\n",
    "from tqdm import tqdm\n",
    "import regex\n",
    "import re\n",
    "import time\n",
    "\n",
    "def process_wiki_xml(f):\n",
    "    containers = []\n",
    "    container = []\n",
    "    for line in f:\n",
    "        if line.startswith(\"<\"):\n",
    "            container = \" \".join(container[1:])\n",
    "            if len(container.split(\" \")) >= 25:\n",
    "                containers.append(container)\n",
    "            container = []\n",
    "            token_count = 0\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if len(line) > 0:\n",
    "            container.append(line)\n",
    "    return containers\n",
    "\n",
    "def read_cpp_res(domain):\n",
    "    tokens = [bytes.fromhex(t.strip()) for t in open(f'cpp_outputs/{domain}/tokens.txt','r').read().strip().split('\\n')]\n",
    "    merges_per_turn = [int(x) for x in open(f'cpp_outputs/{domain}/merges.txt','r').read().strip().split('\\n')]\n",
    "    total = 0\n",
    "    totals = []\n",
    "    for m in merges_per_turn:\n",
    "        total += m\n",
    "        totals.append(total)\n",
    "    return tokens, merges_per_turn, totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9aac83",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "We do some regex pre-processing before passing the parts to greedtok.\n",
    "\n",
    "We can also use std::regex in cpp via:\n",
    "```\n",
    "gb.set_regex_pattern(pat_str)\n",
    "tokenized = gb.batch_split_and_tokenize(original_texts)\n",
    "```\n",
    "However, note that using std::regex is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09509981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trie constructed\n",
      "Number of texts: 73720\n",
      "1192241.8657683365 words/second\n"
     ]
    }
   ],
   "source": [
    "tokens, _, __ = read_cpp_res('wiki')\n",
    "tokens = [bytes([i]) for i in range(256)] + tokens\n",
    "gb = greedy_builder.build_greedy_tokenizer(tokens)\n",
    "\n",
    "orig = [ x for B in 'ABCDE' for i in range(10) for x in process_wiki_xml(open(f\"/data/jiapeng/wiki/cleaned/A{B}/wiki_0{i}\"))]\n",
    "pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "processed = [[re.sub(' ','Ġ',x) for x in regex.findall(pat_str, doc)] for doc in orig]\n",
    "print(\"Number of texts:\", len(orig))\n",
    "\n",
    "start = time.process_time()\n",
    "tokenized = gb.batch_tokenize_in_parts(processed)\n",
    "end = time.process_time() - start\n",
    "print(sum([len(x) for x in processed])/end, \"words/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65fad8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions they claim maintain unnecessary coercion and hierarchy, typically including, though not necessarily limited to, the state and capitalism. Anarchism advocates for the replacement of the state with stateless societies or other forms of free associations. As a historically left-wing movement, usually placed on the farthest left of the political spectrum, it is\n",
      "ideas. The Marxist criticism of anarchism is that it has a utopian character because all individuals should have anarchist views and values. According to the Marxist view, that a social idea would follow directly from this human ideal and out of the free will of every individual formed its essence. Marxists state that this contradiction was responsible for their inability to act. In the anarchist vision, the conflict between liberty and equality was resolved through coexistence and intertwining.\n"
     ]
    }
   ],
   "source": [
    "print(orig[0][:500]) #original text\n",
    "print(orig[0][-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7e07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3138, 2127, 983, 335, 258, 1138, 6680, 263, 2943, 384, 335, 1785, 2878, 484, 261, 629, 1561, 6514, 324, 5266, 263, 2614, 716, 290, 597, 308, 427, 257, 5538, 689, 1140, 4241, 9659, 774, 3879, 289, 263, 286, 880, 8813, 44, 4452, 934, 44, 2286, 514, 6639, 3441, 290, 44, 257, 1176, 263, 2787, 983, 46, 633, 2127, 983, 6367, 628, 324, 257, 7340, 261, 257, 1176, 301, 1697, 6450, 7422, 423, 682, 3656, 261, 2382, 1956, 598, 46, 663, 258, 10230, 1411, 45, 6741, 2943, 44, 2753, 1679, 326, 257, 279, 2351, 371, 1411, 261, 257, 1138, 5516, 6637]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[0][0:100]) #encoding original text up to 100 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a152d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'An', b'arch', b'ism', b'is', b'a', b'political', b'philosophy', b'and', b'movement', b'that', b'is', b'sk', b'ept', b'ical', b'of', b'all', b'just', b'ifications', b'for', b'authority']\n",
      "[b'between', b'liber', b'ty', b'and', b'equ', b'ality', b'was', b'res', b'olved', b'through', b'co', b'ex', b'ist', b'ence', b'and', b'inter', b't', b'win', b'ing', b'.']\n"
     ]
    }
   ],
   "source": [
    "print([tokens[t].strip(b'\\xc4\\xa0') for t in tokenized[0][0:20]]) # token word form \\xc4\\xa0 is Ġ special char\n",
    "print([tokens[t].strip(b'\\xc4\\xa0') for t in tokenized[0][-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57c1ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3494058188772027"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized[0])/len(processed[0]) #token per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d22b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions they claim maintain unnecessary coercion and hierarchy, typically including, though not necessarily limited to, the state and capitalism. Anarchism advocates for the replacement of the state with stateless societies or other forms of free associations. As a historically left-wing movement, usually placed on the farthest left of the political spectrum, it is\n",
      "ideas. The Marxist criticism of anarchism is that it has a utopian character because all individuals should have anarchist views and values. According to the Marxist view, that a social idea would follow directly from this human ideal and out of the free will of every individual formed its essence. Marxists state that this contradiction was responsible for their inability to act. In the anarchist vision, the conflict between liberty and equality was resolved through coexistence and intertwining.\n"
     ]
    }
   ],
   "source": [
    "decoded_example = re.sub('Ġ', ' ', b''.join([tokens[c] for c in tokenized[0]]).decode('utf-8'))\n",
    "print(decoded_example[:500]) #decoded tokens\n",
    "print(decoded_example[-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b15410",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "We can also use any existing BPE merge rules.\n",
    "\n",
    "To bypass std::regex, we iterate through the text as-is without splitting.\n",
    "\n",
    "**Time-complexity is O(W x (max token length) x lgW)**, or simply O(W^2lgW) for convenience.\n",
    "\n",
    "We can choose to ignore regex because the regex pattern should have been imprinted onto the cover rules.\n",
    "\n",
    "Intuitively, this algorithm detects overlapping potential covers and resolves the assignment based on the order of covering priority.\n",
    "\n",
    "For this example, we use the merging keys/rules from cl100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c1b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trie constructed\n",
      "Absolute time: 5.29 second\n",
      "Absolute time: 18425783.90 words/second\n",
      "Relative time: 124.40 second\n",
      "Relative time: 783086.95 words/second\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "rules = list(cl100k_base._mergeable_ranks.keys())\n",
    "\n",
    "gb = greedy_builder.build_greedy_tokenizer(rules)\n",
    "astart, tstart = time.time(), time.process_time()\n",
    "tokenized = gb.batch_tokenize_whole(orig)\n",
    "aend, tend = time.time()-astart, time.process_time() - tstart\n",
    "print(f\"Absolute time: {aend:.2f} second\")\n",
    "print(f\"Absolute time: {sum([len(x) for x in processed])/aend:.2f} words/second\")\n",
    "print(f\"Relative time: {tend:.2f} second\")\n",
    "print(f\"Relative time: {sum([len(x) for x in processed])/tend:.2f} words/second\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c3a0b",
   "metadata": {},
   "source": [
    "On a 2.4Ghz CPU, the algorithm is processing around 750K words/s/thread.\n",
    "\n",
    "Compared to Tiktoken, the above algorithm is _3 times slower_. Nothwithstanding Tiktoken's O(W^2) complexity, they also benefit from Rust's regex crate.\n",
    "\n",
    "Nevertheless, the throughput is reasonable for practical applications, and may improve with future optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a4f8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kildonan—Transcona is an historical electoral division in the Canadian province of Manitoba. It was created for the 1949 provincial election, and eliminated with the 1958 provincial election. Kildonan—Transcona was located to the immediate northwest of Winnipeg, covering such suburban communities as Transcona and East Kildonan. It was won by the socialist Cooperative Commonwealth Federation (CCF) in both 1949 and 1953, but it was not considered a safe seat for the party. Russell Paulley, who ser\n",
      "[42, 699, 263, 276, 2345, 3246, 91524, 374, 459, 13970, 34941, 13096, 304, 279, 12152, 17271, 315, 64340, 13, 1102, 574, 3549, 369, 279, 220, 777, 2491, 36031, 6355, 11, 323, 34373, 449, 279, 220, 777, 2970, 36031, 6355, 13, 735, 699, 263, 276, 2345, 3246, 91524, 574, 7559, 311, 279, 14247, 53342, 315, 52982, 11, 18702, 1778, 46318, 10977, 439, 4149, 91524, 323, 6460, 735, 699, 263, 276, 13, 1102, 574, 2834, 555, 279, 41289, 86805, 38298, 28331, 320, 3791, 37, 8, 304, 2225, 220, 777, 2491, 323, 220, 777, 4331, 11, 719, 433, 574, 539, 6646, 264, 6220]\n",
      "b'Kildonan\\xe2\\x80\\x94Transcona is an historical electoral division in the Canadian province of Manitoba. It was created for the 1949 provincial election, and eliminated with the 1958 provincial election. Kildonan\\xe2\\x80\\x94Transcona was located to the immediate northwest of Winnipeg, covering such suburban communities as Transcona and East Kildonan. It was won by the socialist Cooperative Commonwealth Federation (CCF) in both 1949 and 1953, but it was not considered a safe'\n",
      "1.2483660130718954\n"
     ]
    }
   ],
   "source": [
    "item = 50030\n",
    "print(orig[item][:500]) #original text\n",
    "print(tokenized[item][0:100])\n",
    "print(b''.join([rules[i] for i in tokenized[item][0:100]]))\n",
    "print(len(tokenized[item]) / len(processed[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85961e",
   "metadata": {},
   "source": [
    "### Using the Greedy PCO tokenizer to obtain token sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddad8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpp_counts(domain):\n",
    "    words = [t for t in open(f'cpp_inputs/words/{domain}.txt','r').read().strip().split(' ')]\n",
    "    counts = [int(t) for t in open(f'cpp_inputs/counts/{domain}.txt','r').read().strip().split('\\n')]\n",
    "    return {w:c for w,c in zip(words,counts)}\n",
    "\n",
    "un_counts = get_cpp_counts('un')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d0c8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts size: 105505\n",
      "Token set size: 0\n",
      "Empty token set size selected -> all possible substrings...\n",
      "Final token set size: 884708\n",
      "Initial setup phase: 1759 ms\n"
     ]
    }
   ],
   "source": [
    "# python wrapper for PCO greedy algo\n",
    "# set token_candidates to look at all possible substrings\n",
    "# else specify the exact token candidates\n",
    "token_candidates = set()\n",
    "greedy_tokenizer = greedy_builder.build_greedy_pco_tokenizer(un_counts, token_candidates)\n",
    "# initialize graph\n",
    "greedy_tokenizer.initialize_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c315e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main routine...\n",
      "1. |Ġ [c4 a0 ] | 30035114 | 106 ms | 655 ms | shortlist: 858979\n",
      "2. |Ġthe [c4 a0 74 68 65 ] | 8286735 | 156 ms | 205 ms | shortlist: 3378\n",
      "3. |tion [74 69 6f 6e ] | 4043268 | 41 ms | 87 ms | shortlist: 88654\n",
      "4. |Ġof [c4 a0 6f 66 ] | 3300812 | 45 ms | 47 ms | shortlist: 1045\n",
      "5. |Ġand [c4 a0 61 6e 64 ] | 3262209 | 36 ms | 37 ms | shortlist: 717\n",
      "6. |in [69 6e ] | 2782359 | 36 ms | 154 ms | shortlist: 228318\n",
      "7. |re [72 65 ] | 2384688 | 52 ms | 118 ms | shortlist: 123799\n",
      "8. |Ġt [c4 a0 74 ] | 2299618 | 50 ms | 64 ms | shortlist: 28821\n",
      "9. |Ġa [c4 a0 61 ] | 2171690 | 46 ms | 68 ms | shortlist: 47226\n",
      "10. |er [65 72 ] | 1910147 | 44 ms | 117 ms | shortlist: 149877\n",
      "11. |en [65 6e ] | 1824071 | 50 ms | 112 ms | shortlist: 121023\n",
      "12. |Ġco [c4 a0 63 6f ] | 1782132 | 47 ms | 67 ms | shortlist: 37757\n",
      "13. |it [69 74 ] | 1622191 | 41 ms | 86 ms | shortlist: 79570\n",
      "14. |Ġw [c4 a0 77 ] | 1404713 | 44 ms | 52 ms | shortlist: 13165\n",
      "15. |es [65 73 ] | 1365110 | 39 ms | 110 ms | shortlist: 158470\n",
      "16. |Ġs [c4 a0 73 ] | 1363207 | 47 ms | 75 ms | shortlist: 64252\n",
      "17. |or [6f 72 ] | 1320820 | 45 ms | 83 ms | shortlist: 69488\n",
      "18. |at [61 74 ] | 1291572 | 44 ms | 96 ms | shortlist: 93545\n",
      "19. |is [69 73 ] | 1281159 | 47 ms | 99 ms | shortlist: 105026\n",
      "20. |al [61 6c ] | 1279005 | 47 ms | 105 ms | shortlist: 110311\n",
      "21. |Ġp [c4 a0 70 ] | 1251643 | 49 ms | 75 ms | shortlist: 55276\n",
      "22. |on [6f 6e ] | 1224984 | 48 ms | 98 ms | shortlist: 86292\n",
      "23. |an [61 6e ] | 1187642 | 45 ms | 102 ms | shortlist: 121821\n",
      "24. |Ġin [c4 a0 69 6e ] | 1158876 | 48 ms | 68 ms | shortlist: 39948\n",
      "25. |ed [65 64 ] | 1137380 | 43 ms | 103 ms | shortlist: 123048\n",
      "26. |Ġto [c4 a0 74 6f ] | 1111504 | 47 ms | 52 ms | shortlist: 2796\n",
      "27. |Ġf [c4 a0 66 ] | 980142 | 35 ms | 47 ms | shortlist: 26518\n",
      "28. |Ġbe [c4 a0 62 65 ] | 957046 | 41 ms | 44 ms | shortlist: 4308\n",
      "29. |ation [61 74 69 6f 6e ] | 950042 | 35 ms | 67 ms | shortlist: 68057\n",
      "30. |ic [69 63 ] | 949111 | 42 ms | 78 ms | shortlist: 78523\n",
      "31. |ou [6f 75 ] | 925779 | 52 ms | 80 ms | shortlist: 53570\n",
      "32. |ar [61 72 ] | 877629 | 44 ms | 84 ms | shortlist: 87863\n",
      "33. |ment [6d 65 6e 74 ] | 871122 | 50 ms | 68 ms | shortlist: 27719\n",
      "34. |Ġthat [c4 a0 74 68 61 74 ] | 856960 | 39 ms | 43 ms | shortlist: 457\n",
      "35. |ing [69 6e 67 ] | 814643 | 32 ms | 98 ms | shortlist: 146718\n",
      "36. |Ġdevelop [c4 a0 64 65 76 65 6c 6f 70 ] | 776895 | 48 ms | 50 ms | shortlist: 208\n",
      "37. |Ġm [c4 a0 6d ] | 768147 | 33 ms | 48 ms | shortlist: 38084\n",
      "38. |le [6c 65 ] | 761238 | 39 ms | 63 ms | shortlist: 60724\n",
      "39. |Ġh [c4 a0 68 ] | 732423 | 43 ms | 53 ms | shortlist: 21427\n",
      "40. |Ġre [c4 a0 72 65 ] | 691607 | 37 ms | 52 ms | shortlist: 39427\n",
      "41. |ĠUnited [c4 a0 55 6e 69 74 65 64 ] | 677796 | 40 ms | 41 ms | shortlist: 203\n",
      "42. |Ġd [c4 a0 64 ] | 665422 | 36 ms | 62 ms | shortlist: 52129\n",
      "43. |Ġcountr [c4 a0 63 6f 75 6e 74 72 ] | 647144 | 39 ms | 41 ms | shortlist: 484\n",
      "44. |st [73 74 ] | 639635 | 30 ms | 53 ms | shortlist: 46494\n",
      "45. |Ġinternational [c4 a0 69 6e 74 65 72 6e 61 74 69 6f 6e 61 6c ] | 638585 | 45 ms | 47 ms | shortlist: 767\n",
      "46. |ro [72 6f ] | 583325 | 33 ms | 53 ms | shortlist: 43713\n",
      "47. |ce [63 65 ] | 568796 | 39 ms | 53 ms | shortlist: 30057\n",
      "48. |ve [76 65 ] | 563535 | 38 ms | 51 ms | shortlist: 30416\n",
      "49. |Ġn [c4 a0 6e ] | 555827 | 41 ms | 48 ms | shortlist: 16613\n",
      "50. |Ġwhich [c4 a0 77 68 69 63 68 ] | 543615 | 38 ms | 39 ms | shortlist: 83\n",
      "Total time taken: 4 seconds\n"
     ]
    }
   ],
   "source": [
    "# let's solve for the first 50 steps\n",
    "tokens, scores = greedy_tokenizer.solve_to_step(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b471c6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "958db071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50. |scar [73 63 61 72 ] | 1602\n",
      "51. |edy [65 64 79 ] | 5645\n"
     ]
    }
   ],
   "source": [
    "# add in some manual tokens in between\n",
    "# useful for warm starts, not that we will have to recalculate the whole cache again after\n",
    "tokens, scores = greedy_tokenizer.custom_steps([\"scar\", \"edy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e05a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bcc0478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main routine...\n",
      "53. |Ġe [c4 a0 65 ] | 533333 | 92 ms | 114 ms | shortlist: 26039\n",
      "54. |il [69 6c ] | 507782 | 42 ms | 64 ms | shortlist: 50519\n",
      "55. |Ġc [c4 a0 63 ] | 483397 | 43 ms | 57 ms | shortlist: 30385\n",
      "56. |Ġb [c4 a0 62 ] | 468378 | 40 ms | 51 ms | shortlist: 26087\n",
      "57. |ly [6c 79 ] | 466512 | 40 ms | 72 ms | shortlist: 85549\n",
      "58. |th [74 68 ] | 457339 | 48 ms | 60 ms | shortlist: 25726\n",
      "59. |as [61 73 ] | 457261 | 40 ms | 57 ms | shortlist: 35662\n",
      "60. |ec [65 63 ] | 453637 | 41 ms | 55 ms | shortlist: 27366\n",
      "Total time taken: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "# continue solving till k=60\n",
    "tokens, scores = greedy_tokenizer.solve_to_step(60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3080721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "['Ġ', 'Ġthe', 'tion', 'Ġof', 'Ġand', 'in', 're', 'Ġt', 'Ġa', 'er', 'en', 'Ġco', 'it', 'Ġw', 'es', 'Ġs', 'or', 'at', 'is', 'al', 'Ġp', 'on', 'an', 'Ġin', 'ed', 'Ġto', 'Ġf', 'Ġbe', 'ation', 'ic', 'ou', 'ar', 'ment', 'Ġthat', 'ing', 'Ġdevelop', 'Ġm', 'le', 'Ġh', 'Ġre', 'ĠUnited', 'Ġd', 'Ġcountr', 'st', 'Ġinternational', 'ro', 'ce', 've', 'Ġn', 'Ġwhich', 'scar', 'edy', 'Ġe', 'il', 'Ġc', 'Ġb', 'ly', 'th', 'as', 'ec']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tbb]",
   "language": "python",
   "name": "conda-env-tbb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
